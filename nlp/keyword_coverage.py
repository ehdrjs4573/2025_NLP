"""
keyword_coverage.py

SBERT ê¸°ë°˜ ì˜ë¯¸ ë§¤ì¹­ìœ¼ë¡œ
ì§ë¬´ í•µì‹¬ í‚¤ì›Œë“œ(Skills / Knowledge / Abilities)ê°€
ìê¸°ì†Œê°œì„œ ë¬¸ì¥ì— ì–¼ë§ˆë‚˜ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ë¶„ì„í•˜ëŠ” ëª¨ë“ˆ.
"""

from __future__ import annotations
from typing import Dict, List
import re
import numpy as np

from nlp.embedding import embed_sentences
from nlp.preprocessing import preprocess
from nlp.loaders import load_raw_job_vectors
from nlp.similarity import cosine_similarity


# ==============================
# ì„¤ì •
# ==============================
SIM_THRESHOLD = 0.35  # ğŸ”¥ ë„ë„í•œ ì˜ë¯¸ ë§¤ì¹­ ê¸°ì¤€


# ==============================
# ì§ë¬´ í‚¤ì›Œë“œ ìˆ˜ì§‘
# ==============================
def collect_job_keywords_by_group(job_info: Dict) -> Dict[str, List[str]]:
    """
    ì§ë¬´ ì •ë³´ë¥¼ Skills / Knowledge / Abilities ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¦¬
    """
    groups = {
        "skills": [],
        "knowledge": [],
        "main_abilities": []
    }

    field_map = {
        "skills": "skills",
        "knowledge": "knowledge",
        "main_abilities": "main_abilities"
    }

    for group, field in field_map.items():
        items = job_info.get(field, [])
        for item in items:
            # 'ê³ ì¥ì˜ ë°œê²¬Â·ìˆ˜ë¦¬' ê°™ì€ ë³µí•©ì–´ ë¶„ë¦¬
            parts = re.split(r"[Â·,()/ ]+", item)
            groups[group].extend([p.strip() for p in parts if len(p.strip()) > 1])

    # ì¤‘ë³µ ì œê±°
    for k in groups:
        groups[k] = list(set(groups[k]))

    return groups


# ==============================
# ì˜ë¯¸ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
# ==============================
def semantic_keyword_match(
    keywords: List[str],
    sentences: List[str],
    sentence_embeddings: np.ndarray
) -> Dict:

    if not keywords or not sentences:
        return {
            "matched": {},
            "missing": keywords,
            "coverage_score": 0.0
        }

    keyword_embeddings = embed_sentences(keywords)

    matched = {}
    missing = []

    for idx, kw_vec in enumerate(keyword_embeddings):
        max_sim = 0.0
        max_idx = -1

        # ğŸ”¥ ì—¬ê¸°ì„œ forë¡œ ë¬¸ì¥ í•˜ë‚˜ì”© ë¹„êµ
        for sent_idx, sent_vec in enumerate(sentence_embeddings):
            sim = cosine_similarity(kw_vec, sent_vec)
            if sim > max_sim:
                max_sim = sim
                max_idx = sent_idx

        if max_sim >= SIM_THRESHOLD:
            matched[keywords[idx]] = {
                "sentence": sentences[max_idx],
                "similarity": round(float(max_sim), 3)
            }
        else:
            missing.append(keywords[idx])

    coverage = round(len(matched) / len(keywords) * 100, 2) if keywords else 0.0

    return {
        "matched": matched,
        "missing": missing,
        "coverage_score": coverage
    }
# ==============================
# ì¶”ì²œ ë¬¸ì¥ ìƒì„±
# ==============================
def generate_recommend_phrases(missing_keywords: List[str]) -> List[str]:
    templates = [
        "{}ê³¼(ì™€) ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ê²½í—˜ì„ í–‰ë™ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•´ë³´ì„¸ìš”.",
        "{}ì„(ë¥¼) í™œìš©í•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê±°ë‚˜ íŒë‹¨í–ˆë˜ ì‚¬ë¡€ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”.",
        "{} ì—­ëŸ‰ì´ ë“œëŸ¬ë‚˜ëŠ” ê²°ê³¼ë‚˜ ì„±ê³¼ë¥¼ í•¨ê»˜ ì œì‹œí•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤."
    ]

    recs = []
    for kw in missing_keywords[:5]:
        for t in templates:
            recs.append(t.format(kw))

    return recs


# ==============================
# ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# ==============================
def analyze_keyword_coverage(job_id: int, essay_text: str) -> Dict:
    """
    report_builderì—ì„œ í˜¸ì¶œë˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    # ì§ë¬´ ì •ë³´ ë¡œë“œ
    raw_jobs = load_raw_job_vectors()
    job_info = raw_jobs.get(job_id)

    if job_info is None:
        return {}

    # ì „ì²˜ë¦¬
    prep = preprocess(essay_text)
    sentences = prep["sentences"]

    if not sentences:
        return {}

    sentence_embeddings = embed_sentences(sentences)

    # í‚¤ì›Œë“œ ê·¸ë£¹ ìˆ˜ì§‘
    groups = collect_job_keywords_by_group(job_info)

    group_results = {}
    all_matched = {}
    all_missing = []

    for group_name, keywords in groups.items():
        result = semantic_keyword_match(
            keywords,
            sentences,
            sentence_embeddings
        )

        group_results[group_name] = {
            "coverage_score": result["coverage_score"],
            "matched_keywords": result["matched"],
            "missing_keywords": result["missing"]
        }

        all_matched.update(result["matched"])
        all_missing.extend(result["missing"])

    overall_coverage = round(
        sum(g["coverage_score"] for g in group_results.values()) / len(group_results),
        2
    )

    return {
        "coverage_score": overall_coverage,
        "group_coverage": group_results,
        "matched_keywords": list(all_matched.keys()),
        "missing_keywords": list(set(all_missing)),
        "matched_evidence": all_matched,
        "recommended_phrases": generate_recommend_phrases(all_missing)
    }
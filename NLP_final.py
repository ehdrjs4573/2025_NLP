import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent


# -------------------------
# 1) User-Agent ìƒì„±
# -------------------------
ua = UserAgent()
HEADERS = {
    "User-Agent": ua.random,
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}


# -------------------------
# 2) ê²€ìƒ‰ í•¨ìˆ˜
# -------------------------
def search_cover_letters(company: str, job: str, max_results=10):
    """
    íšŒì‚¬ëª… + ì§ë¬´ëª…ìœ¼ë¡œ ë§ì»¤ë¦¬ì–´ ìì†Œì„œë¥¼ ê²€ìƒ‰í•˜ê³ ,
    ìƒì„¸ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    query = f"{company} {job}"
    url = f"https://linkareer.com/cover-letter/search?query={query}"

    r = requests.get(url, headers=HEADERS)
    if r.status_code != 200:
        print("âŒ ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨:", r.status_code)
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    result_cards = soup.select("a.cover-letter-card")
    results = []

    for card in result_cards[:max_results]:
        title = card.select_one(".title").text.strip() if card.select_one(".title") else "ì œëª© ì—†ìŒ"
        link = card.get("href")

        # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜
        full_url = "https://linkareer.com" + link

        results.append({
            "title": title,
            "url": full_url
        })

    return results


# -------------------------
# 3) ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
# -------------------------
def get_cover_letter_text(url: str):
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ ë¬¸í•­ + ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ê°€ì ¸ì™€
    í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•œë‹¤.
    """
    r = requests.get(url, headers=HEADERS)
    if r.status_code != 200:
        print("âŒ ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨:", r.status_code)
        return ""

    soup = BeautifulSoup(r.text, "html.parser")

    # ìì†Œì„œ ë¬¸í•­ & ë‹µë³€ ì˜ì—­
    qa_blocks = soup.select(".question-item")

    paragraphs = []

    for block in qa_blocks:
        question_el = block.select_one(".question-text")
        answer_el = block.select_one(".answer-text")

        question = question_el.text.strip() if question_el else ""
        answer = answer_el.text.strip() if answer_el else ""

        if question:
            paragraphs.append(f"[ë¬¸í•­] {question}")
        if answer:
            paragraphs.append(f"[ë‹µë³€] {answer}")

    return "\n\n".join(paragraphs)


# -------------------------
# 4) í†µí•© í¬ë¡¤ë§ í•¨ìˆ˜
# -------------------------
def crawl_linkareer(company: str, job: str, limit=5):
    """
    íšŒì‚¬ëª…, ì§ë¬´ëª…ìœ¼ë¡œ ê²€ìƒ‰ â†’ Nê°œ ìì†Œì„œë¥¼ ìƒì„¸ í¬ë¡¤ë§í•´ì„œ
    ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜í•œë‹¤.
    """
    results = search_cover_letters(company, job, max_results=limit)

    cover_letters = []
    for item in results:
        print(f"ğŸ“˜ í¬ë¡¤ë§ ì¤‘: {item['title']}")
        text = get_cover_letter_text(item["url"])

        cover_letters.append({
            "title": item["title"],
            "url": item["url"],
            "text": text
        })

    return cover_letters



# -------------------------
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# -------------------------
if __name__ == "__main__":
    company = "í•œêµ­ì „ë ¥ê³µì‚¬"
    job = "ì „ì‚°ì§"

    data = crawl_linkareer(company, job, limit=3)

    print("\n===== í¬ë¡¤ë§ ê²°ê³¼ =====\n")
    for idx, item in enumerate(data, 1):
        print(f"ğŸ¯ {idx}. {item['title']}")
        print(item["url"])
        print(item["text"][:300], "...\n")  # ì•ë¶€ë¶„ë§Œ ì¶œë ¥